{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e436fa85-e27d-468c-adac-3799c8bfcd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VVAAEE' 'fiber']\n",
      " ['VVVAAAEEE' 'fiber']\n",
      " ['VAEVAE' 'fiber']\n",
      " ['VVVVEE' 'nonfiber']\n",
      " ['AVVVEE' 'fiber']\n",
      " ['EVAAVE' 'fiber']\n",
      " ['WVKAK' 'fiber']\n",
      " ['AWKK' 'fiber']\n",
      " ['YLGSRK' 'fiber']\n",
      " ['IISGKK' 'fiber']\n",
      " ['VSVMDD' 'fiber']\n",
      " ['HIVRR' 'fiber']\n",
      " ['EVEAEE' 'fiber']\n",
      " ['EVEEVE' 'nonfiber']\n",
      " ['LSLDDD' 'fiber']\n",
      " ['DVLDD' 'fiber']\n",
      " ['VILLRK' 'fiber']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Fiber predicting sequences using the ML model \"\"\"\n",
    "\n",
    "import os, time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tqdm\n",
    "\n",
    "\n",
    "################################## ML ARCHITECTURE ################################\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, torch.nn.Conv1d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, torch.nn.LSTM):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\" ML model architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        dropout = 0.4\n",
    "        self.hidden_size = 200\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size, self.hidden_size, self.num_layers, dropout=0, batch_first=True)\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(self.hidden_size, output_size),\n",
    "            torch.nn.Sigmoid()\n",
    "            )\n",
    "            \n",
    "        self.num_params = 0\n",
    "        self.num_params += sum(p.numel() for p in self.lstm.parameters() if p.requires_grad)\n",
    "        self.num_params += sum(p.numel() for p in self.fc.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers,len(x),self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers,len(x),self.hidden_size)\n",
    "        \n",
    "        out, (h, c) = self.lstm(x, (h0, c0))\n",
    "        z = self.fc(out[:,-1,:])\n",
    "        return z\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "################################## MODEL CLASS ####################################\n",
    "\n",
    "class Model():\n",
    "    \"\"\" Main class \"\"\"\n",
    "\n",
    "    def __init__(self, model_path='model.tar'):\n",
    "        # parameters\n",
    "        self.BATCH_SIZE    = 64\n",
    "        self.N_EPOCHS      = 100\n",
    "        self.LR            = 1e-3\n",
    "\n",
    "        self.INPUT_DIM     = 20\n",
    "        self.OUTPUT_DIM    = 1\n",
    "        \n",
    "        self.loss_scale    = 1\n",
    "        self.model_path    = model_path\n",
    "\n",
    "        # Use GPU if available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.model_class = LSTM\n",
    "\n",
    "\n",
    "    def one_hot_encode(self, filename):\n",
    "        \"\"\" one-hot encode the letter sequence \"\"\"\n",
    "        \n",
    "        dataframe = pd.read_csv(filename, sep=',', header=None)\n",
    "        seqs = dataframe.values[:,0]\n",
    "        \n",
    "        y = dataframe.values[:,1]\n",
    "        y = y.reshape(-1,1).astype(float)\n",
    "\n",
    "        # Residue Dictionary\n",
    "        residues = ['G', 'A', 'V', 'S', 'T', 'L', 'I', 'M', 'P', 'F', 'Y', 'W', 'N', 'Q', 'H', 'K', 'R', 'E', 'D', 'C']\n",
    "        res_dict = {}\n",
    "        for i,r in enumerate(residues):\n",
    "            res_dict[r] = i\n",
    "        max_peplen = 10\n",
    "\n",
    "        X = np.zeros((len(seqs), max_peplen, len(res_dict)))\n",
    "        for i,seq in enumerate(seqs):\n",
    "            for j,res in enumerate(seq[:max_peplen]):\n",
    "                X[i,j,res_dict[res]] = 1\n",
    "        \n",
    "        X = X.astype(float)\n",
    "        \n",
    "        print(f\"Sample Distribution: {len(X)} ({np.sum(y)/len(y)})\")\n",
    "\n",
    "        return X, y, seqs\n",
    "\n",
    "\n",
    "    def new_model(self):\n",
    "        self.model = self.model_class(self.INPUT_DIM, self.OUTPUT_DIM).to(self.device)\n",
    "        self.last_loss = 1000\n",
    "        self.last_precision = 0\n",
    "        self.checkpoint = {}\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.checkpoint = torch.load(self.model_path)\n",
    "        self.model = self.model_class(self.INPUT_DIM, self.OUTPUT_DIM).to(self.device)\n",
    "        self.model.load_state_dict(self.checkpoint['last_model_state_dict'])\n",
    "        try:\n",
    "            self.last_precision = checkpoint['last_precision']\n",
    "        except:\n",
    "            self.last_precision = 0\n",
    "        try:\n",
    "            self.last_loss = checkpoint['last_loss']\n",
    "        except:\n",
    "            self.last_loss = 1000\n",
    "\n",
    "\n",
    "    def init_loss_function(self):\n",
    "        self.loss_function = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "\n",
    "    def init_optimizer_function(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "\n",
    "\n",
    "    def train_step(self, train_iterator):\n",
    "        # set the train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # loss of the epoch\n",
    "        train_loss = []\n",
    "\n",
    "        for i, (X, y) in enumerate(train_iterator):\n",
    "            X = X.float()\n",
    "            y = y.float()\n",
    "\n",
    "            X = X.to(self.device)\n",
    "\n",
    "            # update the gradients to zero\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            z = self.model(X)\n",
    "            \n",
    "            # loss\n",
    "            loss = self.loss_function(z, y)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            train_loss += [loss.item()]\n",
    "            \n",
    "            # update the weights\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return np.mean(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, test_iterator):\n",
    "        # set the evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # test loss for the data\n",
    "        test_loss = []\n",
    "\n",
    "        # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "        with torch.no_grad():\n",
    "            for i, (X, y) in enumerate(test_iterator):\n",
    "                X = X.float()\n",
    "                y = y.float()\n",
    "                \n",
    "                X = X.to(self.device)\n",
    "\n",
    "                # forward pass\n",
    "                z = self.model(X)\n",
    "\n",
    "                # loss\n",
    "                loss = self.loss_function(z, y)\n",
    "\n",
    "                # total loss\n",
    "                test_loss += [loss.item()]\n",
    "\n",
    "        return np.mean(test_loss)\n",
    "\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\" Accuracy \"\"\"\n",
    "    \n",
    "        self.model.eval()\n",
    "        z = self.model( torch.tensor(X).float() ).detach().numpy()\n",
    "        \n",
    "        correct = y == np.round(z)\n",
    "        denominator = len(correct)\n",
    "\n",
    "        if len(correct)==0:\n",
    "            accu = 0.0\n",
    "        else:\n",
    "            accu = 100 * np.sum(correct) / denominator\n",
    "        \n",
    "        return accu\n",
    "\n",
    "\n",
    "    def confusion(self, X, y):\n",
    "        \"\"\" confusion matrix calculation \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        z = self.model( torch.tensor(X).float() ).detach().numpy()\n",
    "        z = np.round(z)\n",
    "\n",
    "        P = sum(y)\n",
    "        N = len(y) - P\n",
    "        TP = sum(y*z)\n",
    "        FP = sum((1-y)*z)\n",
    "        TN = sum((1-y)*(1-z))\n",
    "        FN = sum(y*(1-z))\n",
    "\n",
    "        return P, N, TP, FP, TN, FN\n",
    "\n",
    "\n",
    "    def train(self, train_datafile, test_datafile):\n",
    "        # train the model\n",
    "\n",
    "        f = self.one_hot_encode(train_datafile)\n",
    "        X_train, y_train = f[0], f[1]\n",
    "        f = self.one_hot_encode(test_datafile)\n",
    "        X_test, y_test = f[0], f[1]\n",
    "\n",
    "        # optimizer\n",
    "        self.init_optimizer_function()\n",
    "\n",
    "        # init loss function\n",
    "        self.init_loss_function()\n",
    "\n",
    "        # Create iterators for train and test data\n",
    "        train_dataset = Dataset(X_train, y_train)\n",
    "        test_dataset = Dataset(X_test, y_test)\n",
    "        train_iterator = torch.utils.data.DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=False)\n",
    "        test_iterator = torch.utils.data.DataLoader(test_dataset, batch_size=self.BATCH_SIZE)\n",
    "\n",
    "\n",
    "        print(f'Num trainable params: {self.model.num_params}, Num train samples: {len(X_train)}')\n",
    "        accuracy_train = self.accuracy(X_train, y_train)\n",
    "        accuracy_test = self.accuracy(X_test, y_test)\n",
    "        print(f'Accuracy - Train: {accuracy_train:.1f}, Test: {accuracy_test:.1f}')\n",
    "        evaluation = dict(train_loss=[], validation_loss=[], test_loss=[], \n",
    "            accuracy_train=[], accuracy_validation=[], accuracy_test=[])\n",
    "        for e in tqdm.tqdm(range(self.N_EPOCHS)):\n",
    "            train_loss = self.train_step(train_iterator)\n",
    "            test_loss  = self.test_step(test_iterator)\n",
    "\n",
    "            accuracy_train = self.accuracy(X_train, y_train)\n",
    "            accuracy_test  = self.accuracy(X_test, y_test)\n",
    "            P, N, TP, FP, TN, FN = self.confusion(X_test, y_test)\n",
    "            precision = 100 * TP[0] / ( TP[0] + FP[0] )\n",
    "            print(f'Epoch {e:3d} Loss (Accu): Train {train_loss:.3f}({accuracy_train:.1f}), Test {test_loss:.2f}({accuracy_test:.1f}), Precision {precision:.1f}')\n",
    "\n",
    "            evaluation['train_loss'] += [train_loss]\n",
    "            evaluation['test_loss'] += [test_loss]\n",
    "            evaluation['accuracy_train'] += [accuracy_train]\n",
    "            evaluation['accuracy_test'] += [accuracy_test]\n",
    "\n",
    "            # save\n",
    "            self.checkpoint['last_model_state_dict'] = self.model.state_dict()\n",
    "            if test_loss < self.last_loss:\n",
    "                # self.checkpoint['best_model_state_dict'] = copy.deepcopy(self.model.state_dict())\n",
    "                self.checkpoint['loss'] = test_loss\n",
    "                self.last_loss = test_loss\n",
    "            \n",
    "            if precision > self.last_precision:\n",
    "                self.checkpoint['best_model_state_dict'] = copy.deepcopy(self.model.state_dict())\n",
    "                self.checkpoint['precision'] = precision\n",
    "                self.last_precision = precision\n",
    "\n",
    "            torch.save(self.checkpoint, self.model_path)\n",
    "            torch.save(evaluation, 'evaluation.tar')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        z = self.model( torch.tensor(X).float() ).detach().numpy()\n",
    "        return z\n",
    "\n",
    "##################################### LOAD MODEL ##################################\n",
    "def ml_predict(seqs):\n",
    "    model = Model(model_path='model.tar')\n",
    "    model.load_model()\n",
    "    model.model.load_state_dict(model.checkpoint['best_model_state_dict'])\n",
    "    residues = ['G', 'A', 'V', 'S', 'T', 'L', 'I', 'M', 'P', 'F', 'Y', 'W', 'N', 'Q', 'H', 'K', 'R', 'E', 'D', 'C']\n",
    "    res_dict = {}\n",
    "    for i,r in enumerate(residues):\n",
    "        res_dict[r] = i\n",
    "    max_peplen = 10\n",
    "    X = np.zeros((len(seqs), max_peplen, len(res_dict)))\n",
    "    for i,seq in enumerate(seqs):\n",
    "        for j,res in enumerate(seq[:max_peplen]):\n",
    "            X[i,j,res_dict[res]] = 1\n",
    "    z = list(np.round(model.predict(X),1).reshape(-1))\n",
    "    prediction = list(np.array(['nonfiber','fiber'])[np.round(z).astype(int)].reshape(-1))\n",
    "    del model\n",
    "    return np.array(list(zip(seqs,prediction,z)))\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "################################## USER ADD SEQS HERE #############################\n",
    "\n",
    "seqs = [\n",
    "    'VVAAEE', 'VVVAAAEEE',\n",
    "    'VAEVAE', 'VVVVEE', 'AVVVEE', 'EVAAVE', 'WVKAK', 'AWKK', 'YLGSRK', 'IISGKK', \n",
    "    'VSVMDD', 'HIVRR', 'EVEAEE', 'EVEEVE', 'LSLDDD', 'DVLDD', 'VILLRK'\n",
    "    ]\n",
    "\n",
    "\n",
    "print(ml_predict(seqs))\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc384367-dcfa-4217-9e18-62379e9fa8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
